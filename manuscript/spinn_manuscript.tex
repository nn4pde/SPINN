\documentclass[12pt]{article}
\usepackage{newtxtext}

\usepackage[tmargin=1.5in,bmargin=1in,lmargin=1.5in,rmargin=1.25in]{geometry}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{amsmath,amssymb}

\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\graphicspath{{./figures/}}

\usepackage[colorinlistoftodos,textsize=footnotesize]{todonotes}
\newcommand{\todoin}{\todo[inline]}

\usepackage{booktabs}
\usepackage{threeparttable}

\usepackage{xcolor}

\usepackage{lineno}
\modulolinenumbers[1]
\renewcommand\linenumberfont{\color{gray}\tiny\sffamily}

\usepackage{hyperref}
\usepackage{listings}
\lstset{language=Python,
  basicstyle=\ttfamily\small,
  commentstyle=\color{red}\itshape,
  stringstyle=\ttfamily\color{green!50!black},
  showstringspaces=false,
  keywordstyle=\color{blue}\bfseries}

\setlength{\parindent}{0em}
\setlength{\parskip}{1em}
\renewcommand{\baselinestretch}{1.05}

\AtBeginDocument{\addtocontents{toc}{\protect\setlength{\parskip}{0pt}}}

\usepackage{titlesec}

\titleformat*{\section}{\large\bfseries\sffamily}
\titleformat*{\subsection}{\normalsize\bfseries\sffamily}
\titleformat*{\subsubsection}{\normalsize\bfseries\sffamily}

\providecommand{\keywords}[1]{\textbf{\textsf{Keywords:}} #1}

\usepackage{etoolbox}

\makeatletter
\patchcmd{\@maketitle}{\LARGE}{\bfseries\sffamily\large}{}{}
\makeatother

\title{SPINN: Sparse, Physics-based, and Interpretable Neural Networks for PDEs\footnote{Author names listed alphabetically. Both authors contributed equally to the work.}}
\author{\textsf{Amuthan A. Ramabathiran}$^{1,2}$\footnote{Email address: \texttt{amuthan@aero.iitb.ac.in}} \and \textsf{Prabhu Ramachandran}$^{1,2}$\footnote{Email address: \texttt{prabhu@aero.iitb.ac.in}}}
\date{%
	$^1${\small Department of Aerospace Engineering, Indian Institute of Technology Bombay, Mumbai - 400076, Maharashtra, India.}\\[2ex]%
	$^2${\small Center for Machine Intelligence and Data Science (CMINDS), Indian Institute of Technology Bombay, Mumbai - 400076, Maharashtra, India.}\\[2ex]%
	\today
}

\begin{document}
\maketitle

\begin{quote}
\section*{Abstract}
We introduce a class of Sparse, Physics-based, and Interpretable Neural Networks (SPINN) for solving ordinary and partial differential equations. By reinterpreting a traditional meshless representation of solutions of PDEs we develop a class of sparse neural network architectures that are interpretable. The SPINN model we propose here serves as a seamless bridge between two extreme modeling tools for PDEs, dense neural network based methods and traditional mesh-free numerical methods, thereby providing a novel means to develop a new class of hybrid algorithms that build on the best of both these viewpoints. A unique feature of the SPINN model that distinguishes it from other neural network based approximations proposed earlier is that it is (i) fully interpretable, and (ii) sparse in the sense that it has much fewer connections than a dense neural network of the same size. Further, the SPINN algorithm implicitly encodes mesh adaptivity and is able to handle discontinuities in the solutions too. In addition we demonstrate that Fourier series representations can be expressed as a special class of SPINN and propose generalized neural network analogues of Fourier representations. We illustrate the utility of the proposed method with a variety of examples involving ordinary differential equations, elliptic, parabolic, hyperbolic and nonlinear partial differential equations, and an example in fluid dynamics.
\end{quote}


\keywords{Physics-based Neural Networks, Sparse Neural Networks, Interpretable Machine Learning, Partial Differential Equations, Meshless methods, Numerical Methods for PDEs}

%\linenumbers

\section{Introduction}
There has been a flurry of activity in the recent past on the application of machine learning algorithms to solve Partial Differential Equations (PDEs). Unlike traditional methods like the finite element, finite volume, finite difference, and mesh-free methods, Deep Neural Network (DNN) based methods like Physics Informed Neural Networks (PINN) \cite{RPK2019} and the Deep-Ritz method \cite{EYu2018} circumvent the need for traditional mesh-based representations and instead use a DNN to approximate solutions to PDEs. The idea of using DNNs to solve PDEs is not new \cite{LLF97, LLP2000}, but their usage has exploded in the recent past. A non-exhaustive list of other approaches to apply deep learning techniques to solving PDEs include \cite{BN2018, SiKo2018, HJE2018, LLMXD2018, SAGNGHZR2020, WZ2020, LCX2020, CCLL2020, WXZZ2020pre, DS2020, lu2021deepxde, LTPGC2021}. A drawback with such DNN based techniques, apart from their marked inefficiency in comparison with traditional mesh based methods for lower dimensional PDEs, is the fact that they are difficult to interpret and involve many arbitrary choices related to the network architecture.

In this work, we propose a new class of Sparse, Physics-based, and Interpretable Neural Network (SPINN) architectures to solve PDEs that are both interpretable and efficient. DNNs have been studied in the context of meshless methods in works such as \cite{HHM2020, WZ2020}, and connections between DNNs and meshless methods have been noted in works like \cite{EMW20}. The key idea behind SPINN, which distinguishes it from other works cited above, is the observation that certain meshless approximations can be directly transcribed into a sparse DNN. We demonstrate that such a simple re-expression of meshless approximations as sparse DNNs allows us to bridge the emerging field of scientific machine learning and the well established methods of traditional scientific computing.

To set the stage for introducing SPINN, we note that a connection between ReLU DNNs and piecewise linear finite element approximations was proved in \cite{HLXZ2020}. This shows that basis functions with compact support can be represented as a DNN. We generalize this to represent kernel functions in meshless methods as DNNs. We use this to construct the SPINN architecture, which is a new and fully interpretable DNN. This is significant in light of the notorious interpretability problem that attends the use of DNNs. We point out that once the DNN is replaced by a sparse equivalent network based on the aforementioned reinterpretation of meshless methods, we use an approach similar to PINN in modeling the boundary conditions and the loss function directly using the PDE. In addition to providing an interpretable class of neural network architectures, our method also suggests how certain dense networks like PINNs can be interpreted. A further novelty of our method is that it naturally suggests neural network analogues of commonly used transforms such as the Fourier and wavelet transforms. We illustrate how Fourier decomposition can be accomplished using special sparse architectures in one dimension, and suggest natural neural network generalizations that go beyond traditional transformations.

One of the primary advantages of SPINN is that it serves as a natural bridge between traditional meshless methods and methods that use DNNs, including both of these as special cases. This permits us to develop solution methodologies for PDEs that are based on neural networks and yet are computationally competitive with traditional methods. Further, the fact that our proposed method is fully interpretable opens up new roads for traditional computational methods to be enhanced using insights from DNNs, and vice versa. For instance, we also present in this work a new class of implicit finite difference and neural network based solution schemes for time dependent partial differential equations. We thus point out the unique role of SPINN in unifying both traditional and modern numerical methods, while at the same time generalizing them. Further, the fact that SPINN generalizes meshless methods using DNNs facilitates differentiable programming, which is difficult to implement with traditional methods.

The rest of the paper is structured as follows: we begin with an introduction of the SPINN architecture by highlighting the exact relation between certain meshless representations and DNNs. We also show how Fourier representations of functions can be handled in the same framework. We then present a variety of examples involving ordinary and partial differential equations to illustrate the method. We conclude with a discussion of the key ideas presented in this work, along with directions for future investigations. Additional details about some of the simulations are presented in the appendices. The code developed to implement SPINN along with an automation script that reproduces every result reported here can be found at \url{https://github.com/nn4pde/SPINN}.

\section{Methodology}
We present here details of the SPINN architecture in a systematic fashion starting with an illustration of how certain meshless methods using radial basis functions can be exactly represented as a deep neural network. We subsequently use this to develop the general SPINN framework that goes beyond traditional meshless methods, and highlight the precise sense in which SPINN is interpretable. We then provide various details about the way the SPINN architecture is used in solving PDEs. We also propose a generalization of SPINN based on Fourier decomposition.

\subsection{Meshless approximation using radial basis functions}
To set the stage for the introduction of Sparse Physics-based Interpretable Neural Networks (SPINNs), we focus on the problem of finding a solution $u:\Omega \subset \mathbb{R}^d \to \mathbb{R}$ ($d \ge 1$) of the partial differential equation $\mathcal{N}(x, u(x), \nabla u(x), \ldots) = 0$, $x \in \Omega$, with specified Dirichlet and Neumann boundary conditions on the domain boundary $\partial \Omega$. Among the many numerical techniques that have been developed to solve such equations we focus in particular on a class of meshless methods that approximate the solution of the differential equation in terms of Radial Basis Functions (RBFs): for every $x = (x^1, \ldots x^d) \in \Omega$,
\begin{equation} \label{eq:pde_meshless_approx}
u(x) = \sum_{i=1}^N U_i \; \varphi\left(\frac{\lVert x - X_i \rVert}{h_i}\right).
\end{equation}
The various terms in the approximation \eqref{eq:pde_meshless_approx} are to be understood as follows. $(X_i \in \mathbb{R}^d)_{i=1}^N$ represent nodes in the domain $\Omega$. $\varphi:\mathbb{R}\to \mathbb{R}$ represents an RBF kernel. The variables $(h_i \in \mathbb{R})_{i=1}^N$ are appropriately defined measures of width of the RBF kernels centered at the nodes $(X_i)$. Finally, the coefficients $(U_i \in \mathbb{R})_{i=1}^N$ represent the nodal weights associated with the basis functions centered at $(X_i)_{i=1}^N$. In the sequel we also consider a variant of the meshless approximation \eqref{eq:pde_meshless_approx} that enforces the partition of unity property; such an approximation takes the form
\begin{equation} \label{eq:pde_meshless_PoU}
u(x) = \left(\sum_{j=1}^N \varphi\left(\frac{\lVert x - X_j \rVert}{h_j}\right)\right)^{-1}\sum_{i=1}^N U_i  \; \varphi\left(\frac{\lVert x - X_i \rVert}{h_i}\right).
\end{equation}
It is noted that the partition of unity meshless representation \eqref{eq:pde_meshless_PoU} satisfies the important property that it can represent any constant function on $\Omega$ exactly.

\subsection{Meshless approximation reinterpreted as a sparse DNN}
The key idea behind SPINNs is the fact that meshless approximations like \eqref{eq:pde_meshless_approx} and \eqref{eq:pde_meshless_PoU} can be \emph{exactly} represented as specially structured sparse DNN. We first discuss how the meshless approximation \eqref{eq:pde_meshless_approx} can be written as a sparse DNN; the corresponding sparse DNN representation for the partition of unity approximation \eqref{eq:pde_meshless_PoU} is constructed analogously. The meshless approximation \eqref{eq:pde_meshless_approx} can be thought of as a DNN with an architecture as shown in Fig.~\ref{fig:meshless_nn_repr}.

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{images/SPINN.pdf}
\caption{Simplified SPINN architecture.}
\label{fig:meshless_nn_repr}
\end{figure}

We first transform the input  $x \in \Omega$ to the vector $(\lVert x - X_i\rVert/h_i)_{i=1}^N$ via hidden layers which we call the \emph{mesh encoding layer}, shown in blue in Fig.~\ref{fig:meshless_nn_repr}. In more detail, the mesh encoding layer first transforms the input $x \in \Omega$ to a hidden layer with $Nd$ neurons that have input weights
\begin{displaymath}
\left(\underbrace{\frac{1}{h_1}, \ldots, \frac{1}{h_1}}_{\text{$d$ terms}}, \underbrace{\frac{1}{h_2}, \ldots, \frac{1}{h_2}}_{\text{$d$ terms}}, \ldots, \underbrace{\frac{1}{h_N}, \ldots, \frac{1}{h_N}}_{\text{$d$ terms}} \right),
\end{displaymath}
and biases being the $Nd$ vector
\begin{displaymath}
(-X_1^1, \ldots, -X_1^d, -X_2^1, \ldots, -X_2^d, \ldots, -X_N^1, \ldots, -X_N^d),
\end{displaymath}
and with the function $\text{sqr}:\mathbb{R} \to \mathbb{R}$ defined as $\text{sqr}(z) = z^2$ as their activation functions. The output of the first hidden layer of the mesh encoding layer is thus the $Nd$-vector
\begin{displaymath}
\left(\frac{(x - X_1^1)^2}{h_1^2}, \ldots, \frac{(x - X_1^d)^2}{h_1^2}, \frac{(x - X_2^1)^2}{h_2^2}, \ldots, \frac{(x - X_2^d)^2}{h_2^2}, \ldots, \frac{(x - X_N^1)^2}{h_N^2}, \ldots, \frac{(x - X_N^d)^2}{h_N^2}\right).
\end{displaymath}
This is then transformed to another hidden layer consisting of $N$ neurons each of which takes $d$ inputs with weights $1$ and has the function $\text{sqrt}:\mathbb{R} \to \mathbb{R}$ defined as $\text{sqrt}(z) = \sqrt{z}$ as the activation function to produce the $N$-vector
\begin{displaymath}
\left(\frac{\lVert x - X_1\rVert}{h_1}, \frac{\lVert x - X_2\rVert}{h_2}, \ldots, \frac{\lVert x - X_N\rVert}{h_N}\right).
\end{displaymath}
This vector is then passed to the \emph{kernel layer}, shown in brown in Fig.~\ref{fig:meshless_nn_repr}, that consists of $N$ neurons with unit input weights and the the RBF kernel $\varphi$ as the activation function. The outputs of the kernel layer, which is the vector $(\varphi(\lVert x - X_i\rVert/h_i)_{i=1}^N$, is then linearly combined using weights $(U_i)$, which are the coefficients of the meshless approximation \eqref{eq:pde_meshless_approx}, to compute the final output $u(x)$ according to the ansatz \eqref{eq:pde_meshless_approx}. This demonstrates that the meshless ansatz \eqref{eq:pde_meshless_approx} is exactly representable as a DNN with a special architecture as described above. We wish to highlight two important aspects of this architecture: (i) it is sparse; the number of connections and trainable parameters of this network are much smaller than a DNN with the same number of hidden layers and neurons, and (ii) the trainable coefficients of this network, namely the vectors $(h_i)$, $(X_i)$ and $(U_i)$, are interpretable directly in terms of the meshless ansatz \eqref{eq:pde_meshless_approx}.

\begin{figure}
\centering
\includegraphics[width=0.9\textwidth]{images/SPINN_detailed.pdf}
\caption{A detailed view of SPINN with DNN kernel.}
\label{fig:meshless_nn_detailed}
\end{figure}

\subsection{SPINN architecture}
The foregoing discussion naturally motivates the introduction of generalized meshless approximations where the kernel is represented using a DNN. For instance, a partition of unity meshless approximation of the form \eqref{eq:pde_meshless_PoU} with the kernel replaced by a DNN is shown in Figure~\ref{fig:meshless_nn_detailed}. It can be seen that the mesh encoding layers, shown in blue in Figure~\ref{fig:meshless_nn_detailed}, are identical to that described earlier in the context of the DNN equivalent of \eqref{eq:pde_meshless_approx}. The primary difference is that instead of using an RBF kernel as the activation function in the kernel layer, a standard DNN with any differentiable activation function, shown in brown in Figure~\ref{fig:meshless_nn_detailed}, is used as the kernel; we call this the \emph{kernel network} in the sequel. It is worth pointing out that the \emph{same} kernel network is used for each of the outputs of the mesh encoding layer, in conformity with the meshless ansatz \eqref{eq:pde_meshless_PoU}. This drastically reduces the number of connections in the SPINN architecture in comparison with a DNN network having a similar architecture. We therefore see that the SPINN architecture proposed here is more efficient than conventional DNN based methods such as Deep Ritz \cite{EYu2018}, PINN \cite{RPK2019}, etc. Note also that the number of neurons in the mesh encoding layer is exactly the same as the number of nodes used in the meshless discretization. We thus have a physically inspired means to fix the size of the hidden layers in SPINN, unlike other DNN based approaches like PINN where the size of the hidden layers is arbitrary.  Further, except for the parameters of the kernel network, the remaining learnable parameters of the network are fully interpretable. In fact, once the SPINN model is trained, it is straightforward to extract the corresponding meshless ansatz. The parameters of the kernel network do not require to be interpreted since the kernel network itself can be thought of as a generalized RBF kernel.

To emphasize the interpretability of SPINN using as an example the architecture shown in Figure~\ref{fig:meshless_nn_detailed}, which is an instance of the meshless ansatz \eqref{eq:pde_meshless_PoU} with a kernel network in place of the RBF kernel $\varphi$. The interpretability of SPINN is to be understood as follows:

\begin{enumerate}[(i)]
\item The weights connecting the input layer to the first hidden layer encode the widths $(h_i)$ of the meshless kernels located at each node.
\item The biases of the first hidden layer, on the other hand, encode the coordinates $(X_i^1, \ldots, X_i^d)$ of the nodes, which are the centers of the meshless kernels.
\item The kernel network, which in this case is a DNN, stands for the RBF kernel $\varphi$ that the meshless ansatz uses.
\item Finally, the weights of the final connecting the output of the kernel networks to the output layer encodes the coefficients $(U_i)$ of the meshless ansatz.
\end{enumerate}

We thus see that the SPINN network is fully interpretable. In particular, given a SPINN model, it is straightforward to extract a meshless approximation using the procedure elaborated above. This provides a principled rationale for choosing the internal architecture of SPINN based on the corresponding meshless representation. This is in sharp contrast to methods like PINN, Deep Ritz, etc., where the internal architecture of the DNNs used is arbitrary.

We also highlight the fact that since the positions of the nodes and the widths of the kernel associated with the nodes are trainable parameters of the network, the learning algorithm implicitly encodes mesh adaptivity as part of the training process. We note in particular that for problems with large gradients, the widths of the kernels naturally develop a multiscale hierarchy during training, as will be demonstrated in the \emph{Results} section.

\subsection{Kernel functions}
The restriction that the kernel is a radial basis function can be easily removed. It is well known \cite{HLXZ2020} that one-dimensional piecewise linear finite element basis functions can be written exactly in terms of the ReLU basis functions; details are provided in Appendix~\ref{app:relu_fem_1d}. However, ReLU functions are not differentiable, and this can pose problems for their use in ODEs and PDEs of order two or higher. Motivated by the connection between RELU activation functions and hat functions, we propose a new class of basis functions that are infinitely differentiable. We note first that the softplus function
\begin{equation} \label{eq:softplus}
\rho(x) = \log (1 + \exp x),
\end{equation}
provides a smooth approximation of the ReLU function, as shown in Figure~\ref{fig:softplus_relu}.
\begin{figure}[htpb]
\centering
\includegraphics[width=0.5\textwidth]{figures/misc/SoftPlus_Relu.pdf}
\caption{Comparison of the softplus and ReLU functions.}
\label{fig:softplus_relu}
\end{figure}
The softplus function can now be used to create a basis function with \emph{almost} compact support in a manner analogous to the construction of piecewise linear finite element basis functions using ReLU functions. Specifically, we note that the function
\begin{equation} \label{eq:hat_softplus_1d}
N(x) = \frac{1}{\rho(1)}\rho\left(1 + 2\log 2 - \rho(x) - \rho(-x)\right)
\end{equation}
resembles kernels used in meshless approximations; we call this the \emph{softplus hat kernel}. The constants are chosen such that $N(0) = 1$. A graph of the softplus hat function is shown in Figure~\ref{fig:softplus_hat_1d}.

\begin{figure}[htpb]
\begin{subfigure}{0.4\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/misc/SoftPlus_Hat_1d.pdf}
\caption{Softplus hat function in 1D}
\label{fig:softplus_hat_1d}
\end{subfigure}
~
\begin{subfigure}{0.6\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/misc/SoftPlus_Hat_2d.png}
\caption{Softplus hat function in 2D}
\label{fig:softplus_hat_2d}
\end{subfigure}
\caption{Softplus based kernel functions in 1 and 2 dimensions.}
\end{figure}

What makes the softplus hat kernel interesting is the fact that it can be exactly represented as a two layer neural network. The input $x$ first feeds into a hidden layer with two neurons, with weights $(1, -1)$, bias $0$ and softplus activation function. The output of this hidden layer $(\rho(x), \rho(-x))$ is then linearly combined using a second hidden layer with one neuron with weights $(-1, -1)$, bias $1 + 2\log 2$ and softplus activation function. The output of this layer is therefore $\rho\left(1 + 2\log 2 - \rho(x) - \rho(-x)\right)$. Finally, it is straightforward to scale this by a factor of $1/\rho(1)$ to get the final output $N(x)$. We thus see that the softplus hat function is indeed transcribable exactly as a neural network.

It is straightforward to generalize this to higher dimensions. For instance, a $d$ dimensional softplus hat kernel is given by
\begin{equation} \label{eq:hat_softplus_nd}
N(x^1, \ldots, x^d) = \frac{1}{\rho(1)}\rho\left(1 + 2d\log 2 - \sum_{k=1}^d (\rho(x^i) + \rho(-x^i))\right).
\end{equation}
We emphasize that the higher dimensional softplus hat kernel functions \eqref{eq:hat_softplus_nd} are also representable directly as a two layer neural network; this is illustrated for two dimensional softplus hat kernels in Figure~\ref{fig:softplus_hat_nn}. A graph of softplus hat kernel in two dimensions is shown in Figure~\ref{fig:softplus_hat_2d}. Higher dimensional softplus hat functions are also exactly represented by an equivalent neural network.

\begin{figure}
\centering
\includegraphics[width=0.3\textwidth]{images/softplus_hat_nn.pdf}
\caption{Softplus hat kernel represented as a neural network.}
\label{fig:softplus_hat_nn}
\end{figure}

Though the softplus hat kernels do not have compact support, they approach zero quickly outside a small neighborhood of the central node, just like Gaussian kernels. Thus they are expected to have the same performance as Gaussian RBF kernels. This is indeed borne out by the numerical experiments, as will be demonstrated shortly.

We wish to point that the softplus hat kernel is new and has not been used before to the best of the our knowledge. With the choice of these softplus hat kernel functions, meshless approximations that go beyond radial basis functions are constructed along the same lines mentioned before. We present many examples using the softplus hat kernel in the Results section.

\subsection{Loss definition}
With this choice of architecture, solving the PDE, $\mathcal{N}(x, u, \nabla u, \ldots) = 0$, is easily accomplished using a collocation technique similar to the one used in PINNs \cite{RPK2019}. We also note in passing that for PDEs that are obtained as the Euler-Lagrange equations of a known functional, the loss function can be formulated using quadratures of the integral of the functional along with penalty terms to enforce boundary conditions, as is carried out in the Deep Ritz method \cite{EYu2018}.

To illustrate the loss functions used in training the SPINN models, consider the special case of a second order PDE of the form
\begin{displaymath}
\begin{split}
\mathcal{N}(x, u(x), \nabla u(x), \nabla^2 u(x)) = 0, \quad x \in \Omega,\\
u = u_0 \;\text{ on }\partial_0 \Omega, \quad \nabla u \cdot n = g_0 \;\text{ on } \partial \Omega \setminus \partial_0 \Omega,
\end{split}
\end{displaymath}
where $n$ is the outward unit normal to $\partial \Omega$. The generalization to other ODEs and PDEs is straightforward. The loss function for traing the SPINN network for this problem is chosen as
\begin{equation} \label{eq:spinn_loss}
\begin{split}
L((h_i), (X_i), (U_i)) &= w_i \sum_{i=1}^{M_i} \mathcal{N}(\xi_i, u(\xi_i), \nabla u(\xi_i), \nabla^2 u(\xi_i))\\
 &+ w_d \sum_{i=1}^{M_d} (u(\eta_i) - u_0(\eta_i))^2\\
 &+ w_n \sum_{i=1}^{M_n} \left(\nabla u(\zeta_i)\cdot n(\zeta_i) - g_0(\zeta_i)\right)^2.
\end{split}
\end{equation}
Here $w_i$, $w_d$ and $w_n$ are constants that enforce the loss in the interior of the domain, the Dirichlet boundary, and the Neumann boundary, respectively. The set of points $(\xi_i)_{i=1}^{M_i}$, $(\eta_i)_{i=1}^{M_d}$ and $(\zeta_i)_{i=1}^{M_n}$ in the interior of $\Omega$, and on the Dirichlet and Neumann boundaries on $\Omega$, respectively, are the sampling points where the loss is evaluated. In cases where a variational form
\begin{equation} \label{eq:weak_form}
I(u) = \int_{\Omega} f(x, u(x), \nabla u(x))\,dx
\end{equation}
is available for the PDE, the interior loss can alternatively be defined directly in terms of an appropriate quadrature approximation of the integral in \eqref{eq:weak_form}. Dirichlet boundary conditions are imposed using penalty terms as in the strong form collocation case described earlier. Neumann boundary conditions, on the other hand, are directly integrated into the definition of the integral loss functional.

\subsection{Time dependent PDEs}
We present two different approaches for time dependent PDEs using SPINN. The first employs a space-time grid of dimension $d + 1$ and uses exactly the same ideas presented above to solve a time dependent PDE. Alternatively, a hybrid finite difference and SPINN method, which we call FD-SPINN, can be employed which performs time marching using conventional finite difference methods and performs spatial discretization at each time step using the SPINN architecture. It is worth mentioning that both explicit and implicit finite difference schemes are subsumed in FD-SPINN. Both the space-time and the FD-SPINN methods will be illustrated later on.

We illustrate an implicit FD-SPINN algorithm in the special case of the 1D heat equation $u_t = u_{xx}$; a similar formalism applies for other time dependent PDEs. Choosing a time step $\Delta t$, we denote by $(u^k(x))_{k=1}^{N_t}$ the approximations to the solution $u(x, k\Delta t)$; here $N_t$ is chosen such that $N_t \Delta t \simeq T$. The following first order implicit time difference scheme is used in this work:
\begin{displaymath}
\frac{u^{n+1}(x) - u^n(x)}{\Delta t} = \frac{d^2 u^{n+1}(x)}{d x^2}, \quad n = 1, \ldots, N_t.
\end{displaymath}
We wish to emphasize that the spatial derivatives are computed \emph{exactly} using automatic differentiation since the spatial approximation uses SPINN. Thus this implicit scheme is different in comparison to traditional time marching schemes. The loss in the interior of the domain $[0,L]$ is computed as the squared residue of the foregoing equation. The second algorithm uses SPINN for both space and time discretization. The implementation of the space-time SPINN solution is similar to the implementation of second order PDEs described earlier.

\subsection{Fourier-SPINN}
Another advantage of the SPINN architecture is that it suggests natural generalizations of familiar decompositions of functions. To make this precise, consider the Fourier expansion of a function $u:[a,b] \to \mathbb{R}$, namely
\begin{displaymath}
u(x) = a_0 + \sum_{k=1}^{\infty} a_k \cos k\omega x + \sum_{k=1}^{\infty} b_k \sin k\omega x,
\end{displaymath}
where $\omega = 2\pi/(b - a)$. It is straightforward to reinterpret this as a neural network with one hidden layer that has the sinusoidal functions as the activation functions of the neurons. The input $x$ is transformed using a hidden layer with $2N$ neurons to the scaled inputs
\begin{displaymath}
(x, 2x, \ldots, Nx, x, 2x, \ldots, Nx) \in \mathbb{R}^{2N}.
\end{displaymath}
The activation function of the $2N$ neurons in the hidden layer are chosen as
\begin{displaymath}
(\underbrace{\cos \omega z, \cos \omega z, \ldots, \cos \omega z}_{\text{$N$ terms}}, \underbrace{\sin \omega z, \sin \omega z, \ldots, \sin \omega z}_{\text{$N$ terms}}).
\end{displaymath}
The biases of the hidden layer are uniformly set to zero. The output of this hidden layer is then linearly combined to produce the output
\begin{displaymath}
u(x) = U_0 + \sum_{k=1}^N U_k \cos k\omega x + \sum_{k=1}^N V_k \sin k\omega x,
\end{displaymath}
which is just the Fourier representation of $u$. The Fourier representation is then used in conjunction with an appropriately defined loss function to solve a given differential equation for $u$. It bears emphasis that the weights and biases of this neural network are fully interpretable as in the case of the meshless approximation discussed earlier. We note in passing that wavelet transforms could also be represented using SPINN, though we do not pursue that line of study in this paper. As a natural generalization one could replace the sinusoidal functions by a DNN thereby providing a neural network generalization of the Fourier transform.

\subsection{Details of Implementation}
The SPINN architecture proposed in this work is easily implementable using standard NN libraries. We provide PyTorch~\cite{pytorch} implementations of all the examples considered in this work at \url{https://github.com/nn4pde/SPINN}.

We present here certain details of the implementation. We classify the nodes associated with the SPINN model as fixed and free nodes. Fixed nodes are typically used on the (Dirichlet) boundaries. Free nodes, on the other hand, are used inside the domain of definition of the problem. Both fixed and free nodes are designed to have variable kernel widths, but the free nodes are also free to move both inside and outside the domain. A separate set of sampling points on the interior and boundary of the domain are also used. These are the points where the interior and boundary losses are evaluated using the SPINN model. We provide options for both full sampling and random sampling. In random sampling, a random subset of an initially generated set of sampling points is chosen for each iteration of the loss minimization algorithm. The output layer weights and biases are set to zero by default. This implies in particular that when using full sampling and either the Gaussian or softplus hat kernels, the SPINN algorithm is fully deterministic. Thus there are only two sources of stochasticity in the current implementation of SPINN: (i) randomness due to initialization of the DNN kernel weights and biases when DNN kernels are used, and (ii) randomness due to sampling of the interior sampling points when random sampling is used. We do not sample on the boundary and use full boundary sampling always. This is because of the fact that boundary conditions are more delicate to impose in the current framework. The optimization of the SPINN models is carried out using any of the well known optimizers. All the examples presented in this work were carried out using the Adam optimizer \cite{kingma2014} implemented in PyTorch.


\section{Results}
We now present solutions of a variety of ordinary and partial differential equations using SPINN. We implement the SPINN architecture using PyTorch~\cite{pytorch}, and the code is available at \url{https://github.com/nn4pde/SPINN}. All our results are automated using \verb|automan|~\cite{automan:2018} for easy reproducibility.

\subsection{Ordinary differential equations}
To validate the SPINN method, we first consider ordinary differential equations (ODEs) with different boundary conditions. Sample results are shown in Figure~\ref{fig:spinn_ode_3} and Figure~\ref{fig:spinn_ode_2} for ODEs with both Dirichlet and Neumann boundary conditions, respectively; more examples and details of the various simulations are provided in Appendix~\ref{app:ode}. Except for the cases involving the variational and Fourier versions of SPINN, strong form collocation is used to compute the loss function in all examples shown here.

In Figure~\ref{fig:ode3_gaussian_n_3_comp} we present solutions of the ODE
\begin{displaymath}
u''(x) + x(\exp (-(x - (1/3))^2/K) - \exp (-4/9K)) = 0, \quad x\in (0,1),
\end{displaymath}
with zero Dirichlet boundary conditions at $x=0$ and $x=1$ using three different SPINN variants - SPINN with Gaussian kernel and strong form collocation as loss function, variational version of SPINN with Gaussian kernel minimizing the loss functional $I(u) = \int_0^1 (1/2)(u'(x))^2 - f(x)\, dx$, and Fourier-SPINN. The positions of the nodes learnt by the SPINN algorithm are also shown. It is worth pointing out that the nodes adapt to the solution as part of the training process. The convergence of SPINN solutions with a fixed number of nodes but different kernels, as a function of the iteration number, is shown in Figure~\ref{fig:ode3_Linf_n_3}. Gaussian, softplus hat and a DNN with two hidden layers of 5 neurons each are used as the kernels. For all the kernels, it is observed that the error plot shows two distinct regimes. Examining the intermediate solutions during the iteration reveals that the first phase in Figure~\ref{fig:ode3_Linf_n_3} characterized by high loss and slow convergence correlates with the SPINN model minimizing the interior loss. The second phase in Figure~\ref{fig:ode3_Linf_n_3} characterized by a rapid drop in the error correlates with the SPINN model learning the boundary conditions of the ODE. It is further seen that all three kernels perform well, though the Gaussian kernel performs well for this ODE. In obtaining the results in Figure~\ref{fig:ode3_gaussian_n_3_comp} and Figure~\ref{fig:ode3_Linf_n_3}, full sampling is used.

\begin{figure}[htpb]
\begin{subfigure}{0.5\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/ode3_comp/ode3_comp.pdf}
\caption{ODE with Dirichlet boundary conditions.}
\label{fig:ode3_gaussian_n_3_comp}
\end{subfigure}
~
\begin{subfigure}{0.5\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/ode3_conv_3/ode3_Linf_error_n_3.pdf}
\caption{$L_{\infty}$ error for different kernels.}
\label{fig:ode3_Linf_n_3}
\end{subfigure}
\caption{Solution of the ODE $u''(x) + f(x) = 0$, $x \in (0,1)$, $u(0) = u(1) = 0$, where $f(x) = x(\exp (-(x - (1/3))^2/K) - \exp (-4/9K))$ computed using SPINN with Gaussian kernel. The positions of the nodes learnt by SPINN is shown as filled blue circles. The $L_{\infty}$ error associated with different choices of kernels is shown in \ref{fig:ode3_Linf_n_3}.}
\label{fig:spinn_ode_3}
\end{figure}

We present next a solution of the ODE
\begin{displaymath}
u''(x) + \pi^2 u(x) = \pi \sin \pi x, \quad x \in (0,1),
\end{displaymath}
with $u(0) = 0$ and $u'(1) = 1/2$. To handle the Neumann boundary condition at $x=1$, no fixed node is used there and the interior nodes are free to move outside the domain to accommodate the Neumann condition. This is indeed seen in the SPINN solution shown in Figure~\ref{fig:ode2_n_5_f_0p2} where two of the five interior nodes have moved outside the domain $(0,1)$ during the training. When computing the loss, using full sampling does not work in this case as the SPINN algorithm gets trapped in a metastable state which corresponds to an incorrect solution. Random sampling of the interior points, however, provides a convenient means for the SPINN algorithm to escape this metastable state and learn the correct solution. We illustrate the effect of sampling ratio $f$ on the convergence of the SPINN algorithm in Figure~\ref{fig:ode2_n_5_Linf}, thereby demonstrating the significant effect that random sampling has on the convergence of the algorithm for this ODE.

\begin{figure}[htpb]
\begin{subfigure}{0.5\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/ode2_conv_5/ode2_n_5_f_0p2.pdf}
\caption{ODE with Neumann boundary condition and random sampling.}
\label{fig:ode2_n_5_f_0p2}
\end{subfigure}
~
\begin{subfigure}{0.5\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/ode2_conv_5/ode2_Linf_error_n_5_f.pdf}
\caption{$L_{\infty}$ error for different sampling fractions.}
\label{fig:ode2_n_5_Linf}
\end{subfigure}
\caption{Solution of the ODE $u''(x) + \pi^2 u(x) = \pi \sin \pi x$, $x \in (0,1)$, with $u(0) = 0$ and $u'(1) = 1/2$ is shown in \ref{fig:ode2_n_5_f_0p2}. The nodal positions learnt by SPINN are shown as filled blue circles. A random sampling fraction $f=0.2$ is used in this case. The $L_{\infty}$ errors for different sampling fractions are shown in \ref{fig:ode2_n_5_Linf}.}
\label{fig:spinn_ode_2}
\end{figure}

\subsection{PDEs in two dimensions}
We now present a few examples solving PDEs, specifically the Poisson equation in two dimensions. The first example solves the equation
\begin{displaymath}
\nabla^2 u(x, y) = 20\pi^2 \sin 2\pi x \, \sin 4 \pi y,
\end{displaymath}
on the unit square $[0,1]\times[0,1]$ with zero Dirichlet boundary conditions. This equation admits the exact solution,
\begin{displaymath}
u(x,y) = \sin 2\pi x \sin 4 \pi y.
\end{displaymath}
A comparison of the SPINN solution with the exact solution is shown in Figure~\ref{fig:2d_A_softplus_n_100}. The $L_{\infty}$ error as a function of the iteration number is plotted for different kernels in Figure~\ref{fig:2d_A_Linf_n_100}. The kernels chosen here are identical to those considered earlier in the one dimensional case. As before, we observe two distinct regimes in the error graph corresponding to learning the interior and learning the boundary conditions. While all three kernels provide a good solution, the softplus hat kernel performs better than the other two for this PDE. The convergence of the SPINN solution as a function of the number of interior nodes used is shown in Figure~\ref{fig:2d_A_Linf}; we observe that the error decreases with increase in the number of nodes, as expected.

\begin{figure}
\begin{subfigure}{0.32\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/poisson2d_sine_conv/sine2d_softplus_n_100.pdf}
\caption{SPINN solution with softplus hat kernel.}
\label{fig:2d_A_softplus_n_100}
\end{subfigure}
~
\begin{subfigure}{0.32\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/poisson2d_sine_conv/sine2d_Linf_error_n_100.pdf}
\caption{$L_{\infty}$ error as a function of iteration.}
\label{fig:2d_A_Linf_n_100}
\end{subfigure}
~
\begin{subfigure}{0.32\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/poisson2d_sine_nodes/sine2d_Linf_error.pdf}
\caption{$L_{\infty}$ error as a function of number of internal nodes.}
\label{fig:2d_A_Linf}
\end{subfigure}
\caption{SPINN solution of the PDE $\nabla^2 u(x, y) = 20\pi^2 \sin 2\pi x \, \sin 4 \pi y$ on $[0,1]\times[0,1]$. The convergence of the solution with respect to number of internal nodes, and with respect to the number of iterations is also shown.}
\label{fig:spinn_pde2d_sine}
\end{figure}

For the second example we consider the square slit problem governed by the PDE
\begin{displaymath}
\nabla^2 u(x, y) + 1 = 0,
\end{displaymath}
on the domain $[-1,1]\times[-1,1] \setminus [0,1]\times\{0\}$. This PDE does not have an exact solution, but it has known asymptotic properties around the origin. The solution obtained using SPINN is shown in Figure~\ref{fig:poisson_2d_square_slit_sol}. The corresponding nodal positions are shown in Figure~\ref{fig:poisson_2d_square_slit_nodes}. It is seen that the SPINN algorithm learns the optimal position of the nodes and the size of the kernels at the nodal positions appropriately. A comparison of the error of the SPINN solution with respect to a reference finite element solution using a very fine mesh is shown in Figure~\ref{fig:poisson_2d_square_slit_convergence}.

\begin{figure}
\begin{subfigure}{0.32\textwidth}
\includegraphics[width=\textwidth]{figures/square_slit/solution.png}
\caption{SPINN solution of the square slit problem.}
\label{fig:poisson_2d_square_slit_sol}
\end{subfigure}
~
\begin{subfigure}{0.32\textwidth}
\includegraphics[width=\textwidth]{figures/square_slit/sol_centers.png}
\caption{Node and kernel width distribution.}
\label{fig:poisson_2d_square_slit_nodes}
\end{subfigure}
~
\begin{subfigure}{0.32\textwidth}
\includegraphics[width=\textwidth]{figures/square_slit/square_slit_Linf_error.pdf}
\caption{$L_{\infty}$ error as function of number of internal nodes.}
\label{fig:poisson_2d_square_slit_convergence}
\end{subfigure}
\caption{Solution of the square slit problem. The node and kernel width distributions for the solutions learnt by SPINN are shown, along with a plot of the $L_\infty$ error as a function of the number of nodes.}
\label{fig:spinn_pde2d_square_slit}
\end{figure}

The examples shown so far feature domains with regular geometric shapes. The SPINN algorithm, however, works well on arbitrarily shaped domains too. The solution of the Poisson equation $\nabla^2 u(x,y) + 1 = 0$ on an irregularly shaped domain is shown in Figure~\ref{fig:poisson2d_irregular}, and a reference finite element solution computed using a fine mesh is shown in Figure~\ref{fig:poisson2d_irregular_fem}. The distribution of the nodes in this case is shown in Figure~\ref{fig:poisson2d_irregular_nodes}. The $L_{\infty}$ error of the SPINN solution was found to be around $4.9\times 10^{-3}$.

\begin{figure}
\begin{subfigure}{0.32\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/irregular/spinn_solution.png}
\caption{Poisson equation on irregular domain.}
\label{fig:poisson2d_irregular}
\end{subfigure}
~
\begin{subfigure}{0.32\textwidth}
\includegraphics[width=\textwidth]{figures/irregular/sol_centers.png}
\caption{Node and kernel width distribution.}
\label{fig:poisson2d_irregular_nodes}
\end{subfigure}
~
\begin{subfigure}{0.32\textwidth}
\includegraphics[width=\textwidth]{figures/irregular/fem_solution.png}
\caption{Reference FEM solution}
\label{fig:poisson2d_irregular_fem}
\end{subfigure}
\caption{Illustration of the applicability of the SPINN algorithm to solve PDEs defined on complex geometries. Both the nodal positions and reference finite element solutions are shown for comparison.}
\label{fig:spinn_pde2d_irred_dom}
\end{figure}

More details about the SPINN solution of both these PDEs are presented in Appendix~\ref{app:pde}.

\subsection{Heat equation}
We now present examples involving time dependent PDEs. To start with, we consider the one-dimensional heat equation
\begin{displaymath}
\begin{split}
\frac{\partial u(x,t)}{\partial t} = c^2\frac{\partial^2 u(x,t)}{\partial x^2},& \quad x \in (0,L), \; t \in [0, T],\\
u(x, 0) = f(x),& \quad x \in (0,L),\\
u(0,t) = u(L,t) = 0,& \quad t \in [0,T].
\end{split}
\end{displaymath}
We record for reference the exact solution to the heat equation displayed above. The coefficients $(b_k)_{k=1}^{\infty}$ are first defined as the Fourier coefficients of $f$: $f(x) = \sum_{k=1}^{\infty} b_k \sin k\pi x$. The coefficients $(b_k)$ are easily computed as $b_k = \frac{2}{L}\int_{0}^{L} f(x) \sin \frac{n\pi x}{L}\, dx$. The exact solution of the heat equation is then computed as
\begin{displaymath}
u(x,t) = \sum_{k=1}^{\infty} b_k \exp (-\alpha_k^2 t) \sin k\pi x, \quad \alpha_k = \frac{k\pi c}{L}.
\end{displaymath}
The example considered in the paper uses the following inputs: $f(x) = 2\sin \pi x$, $c = 1$, $L = 1$ and $T = 0.1$. The coefficients $(b_k)$ are easily computed as $b_1 = 2$, $b_k = 0, k = 2, 3, \ldots$.

We consider two different methods to solve the heat equation. First, we solve it using the implicit FD-SPINN algorithm
\begin{displaymath}
u^{n+1} = u^n + c^2 \Delta t \, u^{n+1}_{xx}.
\end{displaymath}
It is emphasized that the spatial derivative on the right hand side of the FD-SPINN algorithm displayed above is computed exactly using automatic differentiation, in contrast to typical finite difference schemes which employ a numerical discretization of the derivative operator.

We show time snapshots of the solution in Figure~\ref{fig:heat_eqn_compare}. We also solve this problem as a space-time PDE by emplying SPINN to simultaneously approximate the solution in space and time. The solution is compared with the exact solution in Figure~\ref{fig:heat_eqn_compare} and the space-time solution is shown in Figure~\ref{fig:heat_eqn_st_sol}; the exact solution is also shown as a wireframe for comparison. In Figure~\ref{fig:heat_nodes}, the final positions of the internal nodes learnt by the space-time SPINN and FD-SPINN methods are shown. The code for the space-time version is \verb|code/heat1d.py| and the FD-SPINN version is \verb|code/heat1d_fd_spinn.py|.

\begin{figure}
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=\textwidth]{figures/heat/u_compare.pdf}
\caption{Comparison of exact and SPINN solutions of the heat equation.}
\label{fig:heat_eqn_compare}
\end{subfigure}
~
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=\textwidth]{figures/heat/st_sol.png}
\caption{Space-time solution of the heat equation.}
\label{fig:heat_eqn_st_sol}
\end{subfigure}
\caption{Solution of the one dimensional heat equation using both the space-time version of SPINN and a first order implicit FD-SPINN algorithm.}
\label{fig:heat_eqn}
\end{figure}

\begin{figure}
\begin{subfigure}{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/heat/centers_fd.pdf}
\caption{First order Implicit FD-SPINN.}
\label{fig:heat_fdnodes}
\end{subfigure}
~
\begin{subfigure}{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/heat/centers_st.pdf}
\caption{Space-time SPINN.}
\label{fig:heat_st_nodes}
\end{subfigure}
\caption{Location of the nodes for the heat equation for space-time SPINN and FD-SPINN.}
\label{fig:heat_nodes}
\end{figure}

\subsection{Linear advection equation}
We study next the linear advection equation. The linear advection equation is a classic hyperbolic PDE commonly solved when testing new finite volume algorithms.  We solve the following equation
\begin{displaymath}
\begin{split}
\frac{\partial u}{\partial t} + a \frac{\partial u}{\partial x} &= 0, \quad \quad x \in \mathbb{R}, t \in [0, T],\\
u(x, 0) &= u_0(x), \quad x \in \mathbb{R}\\
u(x, t) &= 0 , \quad |x| \rightarrow \infty.
\end{split}
\end{displaymath}
The exact solution is $u(x, t) = u_0(x - at)$.  We consider a simple Gaussian pulse, $u(x, 0) = e^{-((x + \mu)/2 \sigma)^2}$, where $\mu = 0.3$ and $\sigma=0.15$ and consider the evolution of this, with $a=0.5$ and $T=1$.  Given the almost compact nature of the initial condition we solve the PDE in a finite spatial domain $[-1, 1]$ and over the time interval $[0, 1]$.  Since the initial condition almost vanishes on the boundaries, we set the boundary conditions at $x= -1$ and $x=1$ uniformly to zero.  While this is strictly not correct, we expect the error associated with this to be negligible.  We note however that this restriction can be removed by using the Fourier SPINN model to implement the corresponding spatially periodic model.

As done earlier for the heat equation, we solve the problem using both a first order implicit FD-SPINN as well as a space-time SPINN.  Time snapshots of the solution with a Gaussian pulse as initial condition are shown in Figure~\ref{fig:advection_comp}.  In Figure~\ref{fig:advection_st} the space-time solution is compared with the exact solution, shown in wireframe.  The location of the interior nodes along with the kernel widths is shown in Figure~\ref{fig:advection_nodes}.  The nodes are initially placed uniformly. It is worth emphasizing that the nodes adapt in time and space to capture the features of the solution, which in this case is a travelling wave. We also point out that widths of the kernel are narrow around the peak of the wave while they are broad away from the peak thereby demonstrating mesh-adaptivity. This also illustrates the interpretability of the SPINN algorithm; the position of the nodes and their corresponding widths, as shown in Figure~\ref{fig:advection_nodes} gives an immediately visual representation of the weights and biases of the SPINN model. We would like to point out that such an interpretation is seldom achievable using conventional methods like PINN. The source code for the space-time and FD-SPINN versions of this problem can be found in \verb|code/advection1d.py| and \verb|code/advection1d_fd_spinn.py|, respectively.

\begin{figure}
\begin{subfigure}{0.32\textwidth}
\includegraphics[width=\textwidth]{figures/advection/u_compare.pdf}
\caption{Comparison of exact and SPINN solutions.}
\label{fig:advection_comp}
\end{subfigure}
~
\begin{subfigure}{0.32\textwidth}
\includegraphics[width=\textwidth]{figures/advection/st_sol.png}
\caption{Space-time solution.}
\label{fig:advection_st}
\end{subfigure}
~
\begin{subfigure}{0.32\textwidth}
\includegraphics[scale=0.2]{figures/advection/sol_centers.png}
\caption{Node and kernel width distributions.}
\label{fig:advection_nodes}
\end{subfigure}
\caption{Solution of the linear advection equation using both FD-SPINN and space-time SPINN. The alignment of the nodes and the kernel widths as shown in Figure~\ref{fig:advection_nodes} illustrates both the implicit mesh adaptivity and the interpretability of SPINN.}
\label{fig:linear_advection}
\end{figure}

\subsection{Burgers' equation}
We next consider a classic non-linear, time-dependent hyperbolic PDE in one spatial dimension, namely the inviscid Burgers' equation. The inviscid and viscous Burgers' equations are important hyperbolic PDEs that arise in many applications.  This problem is interesting because it is non-linear, and develops a shock (a discontinuity) in finite time for initially smooth solutions. The inviscid Burgers' equation reads,
\begin{displaymath}
\begin{split}
\frac{\partial u}{\partial t} + u \frac{\partial u}{\partial x} &= 0,  \quad x \in [0, 1], t \in [0, T],\\
u(x, 0) &= \sin(2 \pi x),  \quad x \in [0, 1]\\
u(0, t) = u(1, t) &= 0.
\end{split}
\end{displaymath}
We solve the problem using first order implicit FD-SPINN with 40 internal nodes.  Time snapshots of the solution at different times are shown in Fig.~\ref{fig:burgers_comp} and compared against reference solution obtained using PyClaw~\cite{pyclaw}, a popular and high-quality finite volume package. A shock develops at $x=0$ at around 0.25 seconds which is clearly captured well by SPINN. We show in Figure~\ref{fig:burgers_nodes} the position of the nodes learnt by the FD-SPINN algorithm.  It can be seen that the nodes initially cluster around the peaks of the sine function but cluster around $x=0.5$, which is the location of the shock for $t> 0.2$ thereby demonstrating the nodal adaptivity implicit in the SPINN algorithm. We also point out that the widths of the kernel at the shock locations are much smaller than the corresponding kernels centered at nodes away from the shock. What is remarkable is that despite using smooth kernels, in this case the Gaussian, the FD-SPINN method is able to capture the shock accurately. This thus demonstrates (i) the adaptivity of the SPINN method; the nodes closer to the shock front have a much smaller kernel width in comparison to nodes away from the shock as expected, and (ii) the ability of SPINN to capture discontinuities in the solution. The code for solving the Burgers' equation can be found in \verb|code/burgers1d_fd_spinn.py|.


\begin{figure}
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=\textwidth]{figures/burgers_fd/gaussian_n40.pdf}
\caption{FD-SPINN solution compared to reference PyClaw solution.}
\label{fig:burgers_comp}
\end{subfigure}
~
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=\textwidth]{figures/burgers_fd/centers_fd.pdf}
\caption{Location of nodes for FD-SPINN.}
\label{fig:burgers_nodes}
\end{subfigure}
\caption{Comparison of the implicit FD-SPINN solution of the Burgers' equation with reference solution computed using PyClaw. The location of the nodes is also shown. The shock forms at $x=0.5$; the corresponding clustering of nodes illustrates the adaptivity of the SPINN algorithm.}
\label{fig:inviscid_burgers}
\end{figure}

We also present a space-time solution for the Burgers' equation over the domain $x \in [-1, 1]$ using the following initial condition:
\begin{displaymath}
u_0(x) = \begin{cases}
\sin(\pi(x + 0.75)), & -0.75 \le x \le 0.25,\\
0, & \text{otherwise}.
\end{cases}
\end{displaymath}
This problem also develops a discontinuity. The solution obtained using the space-time SPINN version with a Gaussian kernel is shown in Figure~\ref{fig:spinn_burgers_a} and compared with a reference simulation again obtained using PyClaw. The corresponding distribution of nodes and their widths are shown in Figure~\ref{fig:spinn_burgers_b}. As remarked earlier in the case of linear advection, we see that the nodes align along the shock front, and the widths of the nodes are correspondingly smaller near the shock front to ensure that the shock is captured adequately. Note also that the alignment of the nodes start after $t \simeq 0.5$, which is when the shock forms. It can also be seen that the solution captures the correct shock speed. However, despite the large number of nodes used, we note that there are some oscillations and diffuse behavior near the shock front, unlike the FD-SPINN case. We expect these problems to be solved when the number of nodes is increased, or when a small non-zero artificial viscosity is introduced; we will be investigating this in more detail in a future work. The code for this problem can be found at \verb|code/burgers1d.py|.

\begin{figure}
\centering
\begin{subfigure}{0.45\textwidth}
\includegraphics[width=\textwidth]{figures/burgers_st/gaussian_n435.pdf}
\caption{Space-time SPINN solution.}
\label{fig:spinn_burgers_a}
\end{subfigure}
~
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=\textwidth]{figures/burgers_st/gaussian_n400_centers.png}
\caption{Location and width of nodes.}
\label{fig:spinn_burgers_b}
\end{subfigure}
\caption{Solution of the inviscid Burgers' equation using space-time SPINN with Gaussian kernel and about $400$ nodes.}
\label{fig:spinn_burgers}
\end{figure}

\subsection{Fluid dynamics}
Our final example involves solving the steady incompressible viscous Navier-Stokes equations in two spatial dimensions. We consider the classic lid-driven cavity problem~\cite{ldc:ghia}. Consider a unit square placed on the x-axis in the region $[0, 1] \times [0, 1]$ and filled with a fluid with a density $\rho=1$.   Let the Cartesian velocity components be given by $(u, v)$ and the pressure of the fluid be $p$.  The governing differential equations for the fluid when it attains a steady flow is,
\begin{displaymath}
\begin{split}
\frac{\partial u}{\partial x} + \frac{\partial u}{\partial y} &= 0, \\
u \frac{\partial u}{\partial x} + v \frac{\partial u}{\partial y} &= -\frac{1}{\rho} \frac{\partial p}{\partial x} + \nu \nabla^2 u,\\
u \frac{\partial v}{\partial x} + v \frac{\partial v}{\partial y} &= -\frac{1}{\rho} \frac{\partial p}{\partial y} + \nu \nabla^2 v,\\
\end{split}
\end{displaymath}
where $\nu$ is the kinematic viscosity of the fluid.  The first equation is the conservation of mass (also called the continuity equation) and the subsequent two are the momentum equations. The boundary conditions are given as,
\begin{displaymath}
\begin{split}
    u(x, 0) &= 0\\
    u(x, 1) &= 1\\
    u(0, y) &= 0 \\
    u(1, y) &= 0 \\
    \frac{\partial p}{\partial n} &= 0,
\end{split}
\end{displaymath}
where $n$ is the normal vector along the boundary. The velocity $v$ is zero on the boundary of the square. The walls are modeled with no-slip boundary conditions consistent with the behavior of a viscous fluid. The Reynolds number, $Re$ for this problem is defined as $Re = 1/\nu$. There is no exact solution for this problem but there are many numerical solutions available.  We compare the horizontal and vertical profile of the velocity along the center-line of the square with the classic results of \cite{ldc:ghia} at a Reynolds number of 100.

The solution of this problem requires that SPINN returns a vector representing the solution $(u, v, p)$ at each point where the solution is desired, unlike the previously studied scalar differential equations. The magnitude of the velocity computed using SPINN is shown in Fig.~\ref{fig:ldc_velocity}.  In Fig.~\ref{fig:ldc_ghia_compare} the velocity profile along the center-lines are compared with those of \cite{ldc:ghia} as the number of internal nodes is varied.  We obtain good results with around 225 internal nodes for this problem.  This example was chosen to demonstrate the ability of SPINN to model a non-linear system of PDEs that frequently arise in the modeling physical systems. The corresponding code can be found at \verb|code/cavity.py|.

\begin{figure}
\begin{subfigure}{0.51\textwidth}
\includegraphics[width=\textwidth]{figures/cavity/vmag_n324.pdf}
\caption{Velocity magnitude of flow.}
\label{fig:ldc_velocity}
\end{subfigure}
~
\begin{subfigure}{0.49\textwidth}
\includegraphics[width=\textwidth]{figures/cavity/centerline_compare.pdf}
\caption{Centerline velocity comparison with \cite{ldc:ghia}.}
\label{fig:ldc_ghia_compare}
\end{subfigure}
\caption{Solution of the steady NS equations for the lid-driven cavity problem with a fluid having kinematic viscosity $\nu=0.01$ using SPINN with a Gaussian kernel.}
\label{fig:ldc}
\end{figure}

\section{Discussion}

The primary purpose of the various examples presented earlier is to provide a proof-of-concept demonstration that the SPINN model works well for a large class of ODEs and PDEs. There are however many aspects which can be improved. We discuss some of these here; we will be investigating these systematically in forthcoming publications.

\subsection{Boundary conditions}
For all the variants of SPINN presented in this work, the loss is defined as a weighted sum of the interior and boundary losses. The boundary loss, in particular, is enforced as a penalty term that is proportional to the square of the predicted and imposed boundary values. The constant of proportionality, however, is arbitrarily chosen and varies for each problem. This is a well known limitation of penalty based techniques to enforce constraints. While Dirichlet boundary conditions are relatively easy to enforce, capturing Neumann boundary conditions require careful choice of the fixed and free nodes. For instance, a fixed node at a Neumann boundary when using Gaussian kernels will lead to an infinite indeterminacy of the corresponding coefficient since the slope of the Gaussian kernel is zero at its center. This translates to convergence issues with the loss minimization algorithm. As a simple solution, we use fixed nodes only on the Dirichlet boundaries. We observe that the nodes at times move outside the domain to enforce the boundary condition properly. However, we point out that there are other means to enforce boundary conditions like having a fixed layer of nodes along the boundary, as is done in some particle based methods like Smoothed Particle Hydrodynamics (SPH)~\cite{ye2019sph}. Alternatively, one could \emph{build in} the boundary conditions directly into the solution, as is done in works such as \cite{BN2018}.

\subsection{Time dependent PDEs}
In the context of FD-SPINN algorithms for time dependent PDEs, we used a first order scheme for time marching. However it is in principle straightforward to implement higher order schemes to control the time discretization errors using methods such as those proposed in \cite{SCL2020pre}. We also point out that the FD-SPINN algorithms presented here are different from the corresponding finite difference schemes since the spatial derivatives are handled \emph{exactly} using automatic differentiation. Automatic differentiation has been used in the context of finite element discretization problems in both static \cite{TRRB2002} and dynamic \cite{RG2014} PDEs, but the implicit finite difference schemes we propose here provide a systematic means to develop a variety of efficient time marching schemes.

\subsection{Computational efficiency}
The current implementation of SPINN has been designed as a proof-of-concept.  While the problems considered in this work can be solved with very little computational effort, the implementation will not scale well for larger problems. This is because the method currently evaluates the effect of \emph{all} the nodes on a given sample point making the problem $O(NM)$ given $N$ nodes and $M$ samples.  We have investigated performing spatial convolutions using the \texttt{pytorch-geometric} package~\cite{pytorch_geometric} to accelerate this computation by restricting the interaction of sampling points only with their nearest nodes, thereby reducing the problem to $O(nM)$ where $n$ is the typical number of neighbors.  While preliminary results are encouraging and allow us to use more nodes, there are some limitations and constraints that need to be investigated further.

We note however that SPINN is more efficient than DNN based methods like PINN. To see why, consider a SPINN architecture as shown in Figure~\ref{fig:meshless_nn_detailed}. The number of connections in the SPINN architecture for the particular case illustrated here is\footnote{The actual numbers for the case shown in Figure~\ref{fig:meshless_nn_detailed} are $2dN + (3 + 12 + 12 + 3) + N = (2d + 1)N + 30$, while the number of connections in the corresponding DNN is $(Nd^2 + dN^2 + (3N^2 + 12N^2 + 12N^2 + 3N^2 + N) = (30 + d)N^2 + (d^2 + 1)N$. Here $d$ is the physical dimension of the problem.} $O(N)$ where $N$ is the number of nodes in the meshless representation. This is obtained by counting the number of connections shown in Figure~\ref{fig:meshless_nn_detailed}. Note in particular that since the \emph{same} kernel network is used for each node, the total number of connections for the kernel layer is just that corresponding to a single kernel network. A DNN with same number of hidden layers and same number of neurons in each layer would, on the other hand, have $O(N^2)$ connections, which is \emph{significantly} larger than the number of connections in SPINN. This highlights the sparsity of the SPINN model, and hence the corresponding gain in computational efficiency. Over and above this, we note that by choosing the kernel to be compact functions, SPINN can be made as efficient as a traditional meshless method. This is a significant computational advantage.

\subsection{Wavelet transforms}
The results presented earlier highlight how the SPINN model is able to learn a variety of different length and time scales inherent in a given problem as part of the learning algorithm. A class of classical computational models for multiresolution analysis is wavelet based solutions of PDEs; see for instance \cite{WA94}. It is straightforward to recast wavelet transforms as a SPINN architecture in a manner similar to how the Fourier transform are handled in this work. An advantage of wavelet based representations is that the structure inherent in the scaling and shifting operations in wavelets can be efficiently implemented using spatial convolutions discussed earlier. We wish to reiterate that our current SPINN model already possesses a multiresolution capability on account of the kernel centers and widths being trainable parameters, as is amply demonstrated in the various examples presented.

\subsection{Finite element based extensions of SPINN}
We have used meshless approximations to motivate the development of the SPINN architecture in this work. However, conforming mesh based representations like finite elements could also be used to develop the corresponding neural network generalizations, although the connection is not straightforward. The difficulty arises because of the fact that enforcing mesh conformity places more constraints on the architecture. There are theoretical results elaborating the relation between finite element models and DNNs. For instance in \cite{HLXZ2020} the authors show how every piecewise linear finite element model can be mapped to a ReLU DNN, while in \cite{OPS19} higher order finite elements have been investigated along similar lines.  The advantage in using meshless representations over conforming mesh representations based on the finite elements is that the corresponding DNN has more flexibility in how the nodes move, and how the kernel widths adapt. In addition it also allows for a variety of generalizations like the Fourier and wavelet generalizations of SPINN.

\subsection{Interpreting deep neural network solutions of PDEs}
An interesting consequence of the methodology we present here to reinterpret meshless methods as sparse DNNs is that it also provides a means to interpret certain DNNs such as PINNs~\cite{RPK2019}. Specifically, a DNN approximation of a function $u:\mathbb{R}^d \to \mathbb{R}$ consisting of $N$ hidden layers with $(n_1, n_2, \ldots, n_N)$ neurons in each layer can be thought of as a global approximation of $u$ over its domain in terms of $n_N$ basis functions. If $\sigma$ denotes the activation function of the PINN and $h_k$ denotes the linear function relating the $(k-1)^{\text{th}}$ hidden layer to the $k^{\text{th}}$ hidden layer, $k = 1, \ldots, N$, with $k = 0$ denoting the input layer and $k = N + 1$ denoting the output layer, then it follows immediately from the structure of the network that
\begin{displaymath}
u(x) = w_0 + \sum_{i=1}^{n_N} w_i \,(\sigma \circ h_{N} \circ \sigma \circ h_{N-1} \circ \ldots \circ h_1)_i(x).
\end{displaymath}
Thus denoting by $\phi_i$ the output of the $i^{th}$ neuron in the last hidden layer, we see that the PINN approximation can be written as
\begin{displaymath}
u(x) = w_0 + \sum_{i=1}^{n_N} w_i \phi_i(x).
\end{displaymath}
We thus see that the final layer connection weights and biases can be interpreted as the coefficients of a Ritz type approximation of $u$. Thus using a collocation technique to train the network as is done in PINNs can be thought of as learning global basis functions and their corresponding weights, with the caveat that different nodes have different basis functions. SPINNs, on the other hand are more efficient since they use shifted and scaled versions of a single kernel. They can thus be thought of as an improvement of PINNs by using known information about basis functions to sparsify the network structure and eliminate the arbitrariness that attends the choice of DNN architectures in PINNs.


\section{Conclusion}
To conclude, we have presented a new class of sparse DNNs which we call SPINN - Sparse, Physics-based and Interpretable Neural Networks - which naturally generalize classical meshless methods while retaining the advantages of DNNs. The key features  of SPINN are:
\begin{enumerate}[(i)]
\item It is fully interpretable, in sharp contrast to DNN based methods.
\item It is efficient in comparison with a DNN with the same number of neurons on account of its sparsity.
\item It is physics-based since the loss function depends directly on the strong form of a PDE or its variational form.
\item It implicitly encodes mesh adaptivity as part of its training process.
\item It is able to resolve non-smoothness in solutions, as in the case of shocks.
\item It suggests new classes of numerical algorithms for solving PDEs. For instance, hybrid finite difference SPINN schemes and Fourier-SPINN extensions were illustrated in this work.
\end{enumerate}
We have demonstrated these aspects of SPINN using a variety of ODEs and PDEs, and thus present SPINN as a novel numerical method that seamlessly bridges traditional meshless methods and modern DNN-based algorithms for solving PDEs.  Recognizing this link opens fresh avenues for developing new numerical algorithms that blend the best of both worlds. Finally, even a mere re-expression of meshless algorithms as a SPINN model makes it easier to enhance traditional meshless methods along the lines of differentiable programming.


\section*{Author contributions}
Both authors contributed equally to the conceptualization, formulation of the problem, developing the codes, performing the analyses and writing the manuscript.

\bibliographystyle{plain}
\bibliography{spinn}

\newpage
\section*{Appendix}
\appendix

\section{ReLU networks and piecewise linear finite element approximation} \label{app:relu_fem_1d}
In this section, we illustrate the connection between SPINN and DNN represenations for PDEs that have been been previously studied. To keep the discussion concrete we focus on the special case of one spatial dimension. Consider the problem of minimizing the functional $I:H^1_0([0,1]) \to \mathbb{R}$,
\begin{equation} \label{eq:fnl_1d}
I(u) = \frac{1}{2}\int_0^1 \left(\frac{du(x)}{dx}\right)^2 \,dx - \int_0^1 f(x)u(x) \,dx,
\end{equation}
where $f \in L^2([0,1])$. A standard argument shows that the Euler-Lagrange equations corresponding to the minimization of the functional \eqref{eq:fnl_1d} is the second order ODE:
\begin{equation} \label{eq:ode_2ndorder_1d}
\begin{split}
\frac{d^2u(x)}{dx^2} + f(x) = 0, & \quad x \in [0,1],\\
u(0) = 0, & \quad u(1) = 0.
\end{split}
\end{equation}
To illustrate this, we consider the special case of a piecewise linear finite element approximation of the solution of \eqref{eq:ode_2ndorder_1d}. The basis functions for a piecewise linear finite element approximation can be equivalently thought of as ReLU network with one hidden layer. For this case, a convenient SPINN architecture is the following ReLU network with one hidden layer:
\begin{equation} \label{eq:spinn_1d}
u(x) = \sum_{i=0}^N w_i \text{ReLU}(x - x_i),
\end{equation}
Following \cite{HLXZ2020}, we outline the relation between a piecewise linear finite element representation of a function $u:[a,b] \to \mathbb{R}$ and neural networks with ReLU activation functions. Letting $(x_i)_{i=0}^N$ be a partition of $[a,b]$, such that $x_0 = a$, $x_N = b$, and $x_i < x_{i+1}$ for $0 \le i < N$, the piecewise linear basis function $N_i(x)$, for $1 < i < N$, is given by
\begin{equation} \label{eq:hat_function_fem_1d}
N_i(x) = \begin{cases}
\frac{x_i - x}{x_i - x_{i-1}}, & x \in [x_{i-1},x_i],\\
\frac{x - x_i}{x_{i+1} - x_i}, & x \in [x_i, x_{i+1}].
\end{cases}
\end{equation}
An important observation that connects the finite element approximation of $u$ with ReLU neural networks is the fact that the basis function in \eqref{eq:hat_function_fem_1d} can be written as a linear combination of ReLU functions in the following form:
\begin{equation} \label{eq:hat_1d_relu_repr}
N_i(x) = \frac{1}{h_i}\text{ReLU}(x - x_{i-1}) - \left(\frac{1}{h_i} + \frac{1}{h_{i+1}}\right)\text{ReLU}(x - x_i) + \frac{1}{h_{i+1}}\text{ReLU}(x - x_{i+1}),
\end{equation}
where we use the symbols $h_k = x_{k} - x_{k-1}$, $1 < k < N$, to denote the lengths of the various elements in a given partition of $[a,b]$. Using \eqref{eq:hat_function_fem_1d} and \eqref{eq:hat_1d_relu_repr}, we can write the piecewise linear finite element approximation of $u$, namely
\begin{equation} \label{eq:fem_approx_1d}
u(x) = \sum_{i=0}^N N_i(x) U_i,
\end{equation}
as
\begin{equation} \label{eq:fem_1d_ReLU}
u(x) = \sum_{i=0}^{N - 1} (\theta_{i+1} - \theta_i) \text{ReLU}(x - x_i),
\end{equation}
where $\theta_i = (U_i - U_{i-1})/h_i$. The representation \eqref{eq:fem_1d_ReLU} informs us that a piecewise linear finite element approximation of 1d functions is equivalent to a DNN with one hidden layer with weights and biases consistent with \eqref{eq:fem_1d_ReLU}.

In \cite{HLXZ2020}, the authors proposed a hybrid method wherein they use the representation \eqref{eq:fem_1d_ReLU}, with the weights $(\theta_i)$ computed using the standard finite element method holding the mesh fixed, and the biases $(x_i)$ are computed by minimizing the loss $I$ as a function of the biases $(x_i)$ for fixed values of the weights $(\theta_i)$. This staggered approach, however, does not take full advantage of the variety of stochastic gradient algorithms that have been developed for DNNs. In contrast, the SPINN architecture which we propose in this work does not use a staggered approach, and is more efficient.

Even in this simple case, we note the following features: (i) the weights connecting the input layer, which just takes in $x$, and the hidden layer is $1$ uniformly, (ii) the biases of the hidden layer are directly interpretable as the position of the nodes of the finite element discretization, and (iii) the weights connecting the hidden layer and the output layer are interpretable in terms of the nodal values of the corresponding finite element solution. We also see that the number of neurons in the hidden layer is just the number of interior nodes in the finite element discretization. This is in sharp contrast to the approach followed in Physics Informed Neural Networks (PINN) \cite{RPK2019}, or the Deep-Ritz method \cite{EYu2018}, which employ dense neural networks, and hence are not easily interpretable.

\section{Code design}

\begin{sloppypar}
The source code for SPINN is freely available at \url{https://github.com/nn4pde/SPINN}.  We use the Python programming language, the PyTorch~\cite{pytorch} library, and NumPy~\cite{numpy}.  In addition the code employs the following libraries for visualization and plotting, \verb|matplotlib|~\cite{mpl} is used for the simpler plots and Mayavi~\cite{mayavi} is used for more complex three-dimensional plots.  We use the \verb|pytorch-geometric|~\cite{pytorch_geometric} package to demonstrate the use of geometric convolutions as a means to accelerate the performance, however this is an optional requirement.  Finally, every plot shown in this manuscript is completely automated using the \verb|automan|~\cite{automan:2018} package.
\end{sloppypar}

Our code follows a simple object-oriented design employing the following objects:
\begin{enumerate}[(i)]
\item The \verb|PDE| base class and its sub-classes provide the common methods that one would need to override to define a particular set of ordinary/partial differential equations to solve along with their boundary conditions.
\item Subclasses of \verb|torch.nn.Module| manage the SPINN models.
\item The \verb|Optimizer| class manages the loss optimization.
\item The \verb|Plotter| base class provides routines for live plotting and storing the results.
\item The \verb|App| base class manages the creation and execution of the objects mentioned above to solve a particular problem.
\end{enumerate}

Each problem demonstrated has a separate script which can be executed standalone along with the ability to show the results live. The code is written in a manner such that every important parameter can be configured through the command line.  This is used in our automation script, \verb|automate.py|, which uses the \verb|automan| framework to automate the creation of every figure we present in this work.  We provide the name of the specific scripts for the different differential equations in the sections that follow; these can be found in the \verb|code| sub-directory of the repository. Further, the individual parameters used to obtain the various plots can be found by perusing \verb|automate.py|.  It bears emphasis that every single result presented here is fully reproducible and can be generated by running a single command.

\section{Ordinary differential equations} \label{app:ode}
The following ordinary differential equations, all defined on the domain $[0,1]$, were used to test the SPINN method:
\begin{displaymath}
\begin{split}
\text{1d-A:}\quad & \frac{d^2 u(x)}{dx^2} + 1 = 0, \quad u(0) = u(1) = 0,\\
\text{1d-B:}\quad & \frac{d^2 u(x)}{dx^2} + \pi^2 u(x) = \pi \sin \pi x, \quad u(0) = 0, \frac{du}{dx}(1) = \frac{1}{2},\\
\text{1d-C:}\quad & \frac{d^2 u(x)}{dx^2} + x\left(\exp {\left(-\frac{1}{K}\left(x - \frac{1}{3}\right)^2\right)} - \exp {\left(-\frac{4}{9K}\right)}\right), \; K = 0.01, \quad u(0) = u(1) = 0.
\end{split}
\end{displaymath}
The specific choice of the ODEs were made based on the availability of exact solutions.  The exact solutions to the aforementioned ODEs are as follows:
\begin{displaymath}
\begin{split}
\text{1d-A:}\quad& u(x) = \frac{1}{2}x(1 - x),\\
\text{1d-B:}\quad& u(x) = -\frac{1}{2}x\cos \pi x,\\
\text{1d-C:}\quad& u(x) = -\frac{1}{K^2}\left(K \left(\frac{4}{3} - 6x\right) + 4x\left(x - \frac{1}{3}\right)^2\right)\exp\left(-\frac{1}{K}\left(x - \frac{1}{3}\right)^2\right).
\end{split}
\end{displaymath}
The first two cases, 1d-A and 1d-B, correspond to ODEs with Dirichlet and Neumann boundary conditions, respectively. The third case, 1d-C, features sharp gradients in the solution and thus provides a good test for the SPINN architecture. The python scripts corresponding the ODEs 1d-A, B, C are \verb|ode1.py|, \verb|ode2.py|, and \verb|ode3.py|, respectively.

A comparison of the exact solution and the solution obtained using SPINN for these three ODEs is shown in Figures~\ref{fig:spinn_ode_1}, \ref{fig:spinn_ode_2_A}, and \ref{fig:spinn_ode_3_A}. In addition to the approximations computed by SPINN, these figures also indicate the position of the nodes learnt by the SPINN algorithm. It is worthwhile pointing out that in certain cases, like in Figure~\ref{fig:spinn_ode_2}, some of the nodes move outside the domain.

\begin{figure}
\centering
\begin{subfigure}{0.3\textwidth}
\includegraphics[width=\textwidth]{ode1/ode1_n_1.pdf}
\caption{$n = 1$}
\label{fig:ode1_n_1}
\end{subfigure}
~
\begin{subfigure}{0.3\textwidth}
\includegraphics[width=\textwidth]{ode1/ode1_n_3.pdf}
\caption{$n = 3$}
\label{fig:ode1_n_3}
\end{subfigure}
~
\begin{subfigure}{0.3\textwidth}
\includegraphics[width=\textwidth]{ode1/ode1_n_7.pdf}
\caption{$n = 7$}
\label{fig:ode1_n_7}
\end{subfigure}
\caption{Solution of the ODE $u''(x) + 1 = 0$ in $[0,1]$ with $u(0) = u(1) = 0$ using SPINN with Gaussian kernel using $1$, $3$ and $7$ interior nodes. The nodal positions learnt by SPINN are shown as blue circles along the $x$ axis.}
\label{fig:spinn_ode_1}
\end{figure}

\begin{figure}
\centering
\begin{subfigure}{0.3\textwidth}
\includegraphics[width=\textwidth]{ode2/ode2_n_3.pdf}
\caption{$n = 3$}
\label{fig:ode2_n_3}
\end{subfigure}
~
\begin{subfigure}{0.3\textwidth}
\includegraphics[width=\textwidth]{ode2/ode2_n_5.pdf}
\caption{$n = 5$}
\label{fig:ode2_n_5}
\end{subfigure}
~
\begin{subfigure}{0.3\textwidth}
\includegraphics[width=\textwidth]{ode2/ode2_n_7.pdf}
\caption{$n = 7$}
\label{fig:ode2_n_7}
\end{subfigure}
\caption{Solution of the ODE $u''(x) + \pi^2 u(x) = \pi \sin \pi x$ on $[0,1]$ with boundary conditions $u(0) = 0$ and $u'(x) = 1/2$ using SPINN with Gaussian kernel using $3$, $5$ and $7$ interior nodes. No fixed node was place on the Neumann boundary. The nodal positions learnt by SPINN are shown as blue circles along the $x$ axis.}
\label{fig:spinn_ode_2_A}
\end{figure}

\begin{figure}
\centering
\begin{subfigure}{0.3\textwidth}
\includegraphics[width=\textwidth]{ode3/ode3_n_1.pdf}
\caption{$n = 1$}
\label{fig:ode3_n_1}
\end{subfigure}
~
\begin{subfigure}{0.3\textwidth}
\includegraphics[width=\textwidth]{ode3/ode3_n_3.pdf}
\caption{$n = 3$}
\label{fig:ode3_n_3}
\end{subfigure}
~
\begin{subfigure}{0.3\textwidth}
\includegraphics[width=\textwidth]{ode3/ode3_n_7.pdf}
\caption{$n = 7$}
\label{fig:ode3_n_7}
\end{subfigure}
\caption{Solution of the ODE $u''(x) + x(\exp (-(x - (1/3))^2/K) - \exp (-4/9K)) = 0$ on $[0,1]$, where $K = 0.01$, with boundary conditions $u(0) = u(1) = 0$ using SPINN with Gaussian kernel with $1$, $3$ and $7$ interior nodes. The nodal positions learnt by SPINN are shown as blue circles along the $x$ axis.}
\label{fig:spinn_ode_3_A}
\end{figure}

We now report convergence studies for two of the ODEs specified above - 1d-B and 1d-C. We begin with the simpler case 1d-C. Plots of convergence of $L_1$, $L_2$ and $L_{\infty}$ errors as a function of iteration number are shown in Figure~\ref{fig:ode_1d_C_n_1} with $n=1$ interior nodes and in Figure~\ref{fig:ode_1d_C_n_3} with $n=3$ interior nodes. The corresponding solution obtained by using the SPINN model along with the positions of the nodes learnt by the algorithm are also shown in the figure. As discussed in the main text, two different regimes are observed in the convergence plots corresponding to learning the interior solution and enforcing the boundary conditions.

\begin{figure}
\begin{subfigure}{0.32\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/ode3_conv_1/ode3_gaussian_n_1.pdf}
\caption{Gaussian kernel}
\label{fig:ode3_gaussian_n_1}
\end{subfigure}
~
\begin{subfigure}{0.32\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/ode3_conv_1/ode3_softplus_n_1.pdf}
\caption{Softplus hat kernel}
\label{fig:ode3_softplus_n_1}
\end{subfigure}
~
\begin{subfigure}{0.32\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/ode3_conv_1/ode3_kernel_n_1.pdf}
\caption{Neural network kernel}
\label{fig:ode3_kernel_n_1}
\end{subfigure}
\begin{subfigure}{0.32\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/ode3_conv_1/ode3_L1_error_n_1.pdf}
\caption{$L_1$ error}
\label{fig:ode3_L1_n_1}
\end{subfigure}
~
\begin{subfigure}{0.32\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/ode3_conv_1/ode3_L2_error_n_1.pdf}
\caption{$L_2$ error}
\label{fig:ode3_L2_n_1}
\end{subfigure}
~
\begin{subfigure}{0.32\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/ode3_conv_1/ode3_Linf_error_n_1.pdf}
\caption{$L_{\infty}$ error}
\label{fig:ode3_Linf_n_1}
\end{subfigure}
\caption{Convergence study for ODE 1d-C. The SPINN models have a single internal node in conjunction with three different kernels: Gaussian, softplus hat, and a deep neural network with tanh activation function and with an input layer consisting of 1 neuron, 2 hidden layers with 5 neurons each and an output layer with one neuron. The $L_1$, $L_2$ and $L_{\infty}$ errors of the SPINN solution, computed with respect to the known exact solution, are also shown.}
\label{fig:ode_1d_C_n_1}
\end{figure}

\begin{figure}
\begin{subfigure}{0.32\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/ode3_conv_3/ode3_gaussian_n_3.pdf}
\caption{Gaussian kernel}
\label{fig:ode3_gaussian_n_3}
\end{subfigure}
~
\begin{subfigure}{0.32\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/ode3_conv_3/ode3_softplus_n_3.pdf}
\caption{Softplus hat kernel}
\label{fig:ode3_softplus_n_3}
\end{subfigure}
~
\begin{subfigure}{0.32\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/ode3_conv_3/ode3_kernel_n_3.pdf}
\caption{Neural network kernel}
\label{fig:ode3_kernel_n_3}
\end{subfigure}
\begin{subfigure}{0.32\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/ode3_conv_3/ode3_L1_error_n_3.pdf}
\caption{$L_1$ error}
\label{fig:ode3_L1_n_3}
\end{subfigure}
~
\begin{subfigure}{0.32\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/ode3_conv_3/ode3_L2_error_n_3.pdf}
\caption{$L_2$ error}
\label{fig:ode3_L2_n_3}
\end{subfigure}
~
\begin{subfigure}{0.32\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/ode3_conv_3/ode3_Linf_error_n_3.pdf}
\caption{$L_{\infty}$ error}
\label{fig:ode3_Linf_n_3_full}
\end{subfigure}
\caption{Convergence study for ODE 1d-C. The SPINN models have three internal nodes in conjunction with three different kernels: Gaussian, softplus hat, and a deep neural network with tanh activation function and with an input layer consisting of 1 neuron, 2 hidden layers with 5 neurons each and an output layer with one neuron. The $L_1$, $L_2$ and $L_{\infty}$ errors of the SPINN solution, computed with respect to the known exact solution, are also shown.}
\label{fig:ode_1d_C_n_3}
\end{figure}

We next discuss the effect of random sampling on the performance of the algorithm by considering the second ODE 1d-B. Specifically we vary the fraction of sampling points that are used to evaluate the loss. Choosing the number of nodes $n=5$, but with only the Dirichlet points fixed, the solution obtained using SPINN with Gaussian kernel is shown in Figure~\ref{fig:ode_1d_B_n_6_f}. The corresponding errors are shown in Figure~\ref{fig:ode_1d_B_n_6_errors}.

\begin{figure}
\begin{subfigure}{0.32\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/ode2_conv_5/ode2_n_5_f_0p1.pdf}
\caption{$f = 0.1$}
\label{fig:ode2_n_5_f_0p1}
\end{subfigure}
~
\begin{subfigure}{0.32\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/ode2_conv_5/ode2_n_5_f_0p2.pdf}
\caption{$f=0.2$}
\label{fig:ode2_n_5_f_0p2_a}
\end{subfigure}
~
\begin{subfigure}{0.32\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/ode2_conv_5/ode2_n_5_f_0p3.pdf}
\caption{$f=0.3$}
\label{fig:ode2_n_5_f_0p3}
\end{subfigure}
\begin{subfigure}{0.32\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/ode2_conv_5/ode2_n_5_f_0p5.pdf}
\caption{$f=0.5$}
\label{fig:ode2_n_5_0p5}
\end{subfigure}
~
\begin{subfigure}{0.32\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/ode2_conv_5/ode2_n_5_f_0p75.pdf}
\caption{$f=0.75$}
\label{fig:ode2_n_5_0p75}
\end{subfigure}
~
\begin{subfigure}{0.32\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/ode2_conv_5/ode2_n_5_f_1p0.pdf}
\caption{$f=1$}
\label{fig:ode2_n_5_1}
\end{subfigure}
\caption{Convergence study for ODE 1d-B. SPINN with Gaussian kernel is used to generate all these figures. The number of nodes is fixed at $n=5$ and the total number of samples is fixed at $n_s = 20n$. The various graphs shown here correspond to different fractions, $f$, of the samples used to evaluate the loss at each iteration.}
\label{fig:ode_1d_B_n_6_f}
\end{figure}

\begin{figure}
\begin{subfigure}{0.32\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/ode2_conv_5/ode2_L1_error_n_5_f.pdf}
\caption{$L_1$ error}
\label{fig:ode2_n_5_L1}
\end{subfigure}
~
\begin{subfigure}{0.32\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/ode2_conv_5/ode2_L2_error_n_5_f.pdf}
\caption{$L_2$ error}
\label{fig:ode2_n_5_L2}
\end{subfigure}
~
\begin{subfigure}{0.32\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/ode2_conv_5/ode2_Linf_error_n_5_f.pdf}
\caption{$L_{\infty}$ error}
\label{fig:ode2_n_5_Linfa}
\end{subfigure}
\caption{Convergence of errors for ODE 1d-B. The sampling ratio $f$ is varied while keeping the number of internal nodes and total number of sampling points fixed.}
\label{fig:ode_1d_B_n_6_errors}
\end{figure}

\subsection{Variational implementation of SPINN}
SPINNs can also be designed with variational principles directly if they are available. As a proof of concept, we recall from basic variational calculus that the Euler-Lagrange equation of the functional
\begin{displaymath}
I(u) = \int_0^1 \frac{1}{2} \left(\frac{du(x)}{dx}\right)^2 \, dx - \int_0^1 f(x) u(x) \, dx,
\end{displaymath}
where $u:[0,1]\to\mathbb{R}$ with suitable regularity and such that $u(0) = u(1) = 0$, is precisely the differential equation
\begin{displaymath}
\frac{d^2u (x)}{dx^2} + f(x) = 0,
\end{displaymath}
defined over the domain $(0,1)$ with zero Dirichlet boundary conditions. The functional associated with the differential equation is chosen as the loss function. Since the functional is defined via integrals, an appropriate choice of quadrature is required. For the examples shown in the text a simple Riemann sum over a uniform partition of the interval $[0,1]$ is used to evaluate the integral. Basic examples are shown in Figure~\ref{fig:var_spinn_1d}. Python scripts for the variational SPINN implementations of ODE 1d-A and 1d-C are  \verb|code/ode1_var.py| and \verb|code/ode3_var.py|, respectively.

\begin{figure}
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=\textwidth]{figures/ode1_var/ode1_var_n_5.pdf}
\caption{$f(x) = 1$}
\label{var_spinn_1d_basic}
\end{subfigure}
~
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=\textwidth]{figures/ode3_var/ode3_var_n_5.pdf}
\caption{$f(x) = x(\exp (-(x - (1/3))^2/K) - \exp (-4/9K))$}
\label{var_spinn_1d_bump}
\end{subfigure}
\caption{SPINN solution of ODEs of the form $u''(x) + f(x) = 0$ on $[0,1]$ with $u(0) = u(1) = 0$ by using the variational form $I(u) = \int_0^1 \frac{1}{2}(u'(x))^2 - f(x)u(x) \; dx$ as the loss function. The figure on the left corresponds to the case $f(x) = 1$ and the one on the right corresponds to $f(x) = x(\exp (-(x - (1/3))^2/K) - \exp (-4/9K))$. In both cases $10$ internal nodes were used. The exact solution is also shown for comparison.}
\label{fig:var_spinn_1d}
\end{figure}

\subsection{Fourier SPINN}
\label{app:fourier_spinn}

Preliminary results showing the solution of ODES 1d-A and 1d-C using Fourier SPINN with strong form collocation and Gaussian kernel is shown in Figure~\ref{fig:fourier_spinn_1d}. The python scripts for the Fourier-SPINN implementation of ODEs 1d-A and 1d-C are \verb|code/ode1_fourier.py| and \verb|code/ode3_fourier.py|, respectively.

\begin{figure}
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=\textwidth]{figures/ode1_fourier/ode1_fourier_m_10.pdf}
\caption{$10$ Fourier modes}
\label{fig:fourier_spinn_1d_basic}
\end{subfigure}
~
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=\textwidth]{figures/ode3_fourier/ode3_fourier_m_50.pdf}
\caption{$50$ Fourier modes}
\label{fig:fourier_spinn_1d_bump}
\end{subfigure}
\caption{Fourier-SPINN solutin of ODEs of the form $u''(x) + f(x) = 0$ on $[0,1]$ with $u(0) = u(1) = 0$. The figure on the left corresponds to the case $f(x) = 1$ with $10$ Fourier modes. The one on the right corresponds to $f(x) = x(\exp (-(x - (1/3))^2/K) - \exp (-4/9K))$ with $35$ Fourier modes. The exact solution is also shown for comparison. The loss function for the Fourier-SPINN network is chosen as the energy functional $I(u) = \int_0^1 \frac{1}{2}(u'(x))^2 - f(x)u(x) \; dx$.}
\label{fig:fourier_spinn_1d}
\end{figure}

\section{Partial differential equations} \label{app:pde}
The python scripts corresponding to the PDEs involving the sinusoidal forcing and the square slit problem are \verb|poisson2d_sine.py| and \verb|poisson2d_square_slit.py|, respectively.

We first present the convergence of SPINN with different activation functions with around $100$ internal points and $400$ sampling points in Figure~\ref{fig:sine2d_conv_activation}. We notice the same trend in the convergence as before with an initial slow convergence where SPINN minimizes the interior loss and a second phase where SPINN minimizes the boundary loss. The convergence of errors of the SPINN model with softplus hat kernel as a function of the number of internal nodes is shown in Figure~\ref{fig:sine2d_errors}. It is seen that the error reduces as the number of internal nodes increases, as expected.

\begin{figure}
\begin{subfigure}{0.32\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/poisson2d_sine_conv/sine2d_gaussian_n_100.pdf}
\caption{Gaussian kernel}
\label{fig:2d_A_gaussian_n_100}
\end{subfigure}
~
\begin{subfigure}{0.32\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/poisson2d_sine_conv/sine2d_softplus_n_100.pdf}
\caption{Softplus hat kernel}
\label{fig:2d_A_softplus_n_100_a}
\end{subfigure}
~
\begin{subfigure}{0.32\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/poisson2d_sine_conv/sine2d_kernel_n_100.pdf}
\caption{Neural network kernel}
\label{fig:2d_A_kernel_n_100}
\end{subfigure}
\begin{subfigure}{0.32\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/poisson2d_sine_conv/sine2d_L1_error_n_100.pdf}
\caption{$L_1$ error}
\label{fig:2d_A_L1_n_100}
\end{subfigure}
~
\begin{subfigure}{0.32\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/poisson2d_sine_conv/sine2d_L2_error_n_100.pdf}
\caption{$L_2$ error}
\label{fig:2d_A_L2_n_100}
\end{subfigure}
~
\begin{subfigure}{0.32\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/poisson2d_sine_conv/sine2d_Linf_error_n_100.pdf}
\caption{$L_{\infty}$ error}
\label{fig:2d_A_Linf_n_100_a}
\end{subfigure}
\caption{Convergence study for PDE $\nabla^2 u(x, y) = 20\pi^2 \sin 2\pi x \, \sin 4 \pi y$. The SPINN model has around $200$ internal nodes and $400$ sampling points. Three different kernels: Gaussian, softplus hat, and a deep neural network with tanh activation function and with an input layer consisting of 1 neuron, 2 hidden layers with 5 neurons each and an output layer with one neuron, are used. The $L_1$, $L_2$ and $L_{\infty}$ errors of the SPINN solution, computed with respect to the known exact solution, are also shown.}
\label{fig:sine2d_conv_activation}
\end{figure}

\begin{figure}
\begin{subfigure}{0.32\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/poisson2d_sine_nodes/sine2d_L1_error.pdf}
\caption{$L_1$ error}
\label{fig:2d_A_L1}
\end{subfigure}
~
\begin{subfigure}{0.32\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/poisson2d_sine_nodes/sine2d_L2_error.pdf}
\caption{$L_2$ error}
\label{fig:2d_A_L2}
\end{subfigure}
~
\begin{subfigure}{0.32\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/poisson2d_sine_nodes/sine2d_Linf_error.pdf}
\caption{$L_{\infty}$ error}
\label{fig:2d_A_Linf_a}
\end{subfigure}
\caption{Convergence of errors for PDE $\nabla^2 u(x, y) = 20\pi^2 \sin 2\pi x \, \sin 4 \pi y$ as a function of number of interior nodes. Softplus hat kernel is used for all the SPINN models, and the number of sampling points is chosen as 4 times the number of internal nodes.}
\label{fig:sine2d_errors}
\end{figure}

For the square slit problem, a comparison of the solution computed using SPINN is compared with a solution computed using the finite element method was presented in the main text. The mesh for the finite element solution was created using Gmsh \cite{gmsh}. The finite element solution was computed using Fenics \cite{AlnaesBlechta2015a, LoggMardalEtAl2012a}. The source code for both the mesh generation and the finite element solution can be found inside the \verb|code/mesh_data| and \verb|code/fem| directories respectively.

Finally, we also demonstrate the applicability of the SPINN model to study PDEs defined on arbitrary domains by solving the Laplacian equation $\nabla^2 u(x,y) + 1 = 0$ over an irregular domain, as described in the main text. The source code for this example can be found at  \verb|code/poisson2d_irreg_dom.py| in the repository.

\end{document}